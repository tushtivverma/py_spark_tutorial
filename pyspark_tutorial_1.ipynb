{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db5a8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 317.3 MB 570 bytes/s  0:00:011\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 122.0 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812389 sha256=3eab92bbadcdd91b7b31f2cbaa695f345056cbfd313bec44282ade3af427f9f8\n",
      "  Stored in directory: /Users/tushtivinodverma/Library/Caches/pip/wheels/11/67/ea/33c283e520b775aa7a7a0d404447e287be841a711d074d4d91\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41dacd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977337bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9ec86",
   "metadata": {},
   "source": [
    "## Starting a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a33b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 14:12:29 WARN Utils: Your hostname, Tushtis-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.4.37 instead (on interface en0)\n",
      "24/08/13 14:12:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/13 14:12:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "508808f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.4.37:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8bd075b730>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9704ffcd",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6dec92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f261f36",
   "metadata": {},
   "source": [
    "## Show the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9686d90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  Name|Age|Experience|\n",
      "+------+---+----------+\n",
      "|Tushti| 25|         2|\n",
      "|  Neha| 29|         5|\n",
      "|Swathi| 24|         1|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5410e790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e98c79",
   "metadata": {},
   "source": [
    "## Datatypes of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "010670bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int'), ('Experience', 'int')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ce865",
   "metadata": {},
   "source": [
    "## Get Column Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ce53823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c0569",
   "metadata": {},
   "source": [
    "## Top 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e63bc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Tushti', Age=25, Experience=2),\n",
       " Row(Name='Neha', Age=29, Experience=5),\n",
       " Row(Name='Swathi', Age=24, Experience=1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e33c7d",
   "metadata": {},
   "source": [
    "## Print the column name and it's content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b92ff62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  Name|\n",
      "+------+\n",
      "|Tushti|\n",
      "|  Neha|\n",
      "|Swathi|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a8814",
   "metadata": {},
   "source": [
    "## Print Multiple Column Names and their content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6363f206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|  Name|Experience|\n",
      "+------+----------+\n",
      "|Tushti|         2|\n",
      "|  Neha|         5|\n",
      "|Swathi|         1|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name','Experience']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd745a0",
   "metadata": {},
   "source": [
    "## Basic information of all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aaeacbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+------------------+\n",
      "|summary|  Name|               Age|        Experience|\n",
      "+-------+------+------------------+------------------+\n",
      "|  count|     3|                 3|                 3|\n",
      "|   mean|  NULL|              26.0|2.6666666666666665|\n",
      "| stddev|  NULL|2.6457513110645907|2.0816659994661326|\n",
      "|    min|  Neha|                24|                 1|\n",
      "|    max|Tushti|                29|                 5|\n",
      "+-------+------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950719ad",
   "metadata": {},
   "source": [
    "## Adding Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5bb362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------------------------+\n",
      "|  Name|Age|Experience|Experience after 2 years|\n",
      "+------+---+----------+------------------------+\n",
      "|Tushti| 25|         2|                       4|\n",
      "|  Neha| 29|         5|                       7|\n",
      "|Swathi| 24|         1|                       3|\n",
      "+------+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumn('Experience after 2 years', df_pyspark['Experience']+2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee426659",
   "metadata": {},
   "source": [
    "## Changing Column Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80dc6e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|FullName|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|  Tushti| 25|         2|\n",
      "|    Neha| 29|         5|\n",
      "|  Swathi| 24|         1|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumnRenamed('Name','FullName').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2546f9c3",
   "metadata": {},
   "source": [
    "## Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54a601fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_pyspark.withColumn('Experience after 2 years', df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9489e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------------------------+\n",
      "|  Name|Age|Experience|Experience after 2 years|\n",
      "+------+---+----------+------------------------+\n",
      "|Tushti| 25|         2|                       4|\n",
      "|  Neha| 29|         5|                       7|\n",
      "|Swathi| 24|         1|                       3|\n",
      "+------+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c09510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Experience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dadaf281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------------------------+\n",
      "|  Name|Age|Experience after 2 years|\n",
      "+------+---+------------------------+\n",
      "|Tushti| 25|                       4|\n",
      "|  Neha| 29|                       7|\n",
      "|Swathi| 24|                       3|\n",
      "+------+---+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7216d66",
   "metadata": {},
   "source": [
    "## Read a new Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ea2ad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.read.csv('test2.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3352e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+\n",
      "|      Name| Age|Experience|Salary|\n",
      "+----------+----+----------+------+\n",
      "|    Tushti|  25|         3| 90000|\n",
      "|      Neha|  29|         5|118000|\n",
      "|    Swathi|  24|         1| 85000|\n",
      "|     Rohan|  25|         2| 98000|\n",
      "|  Ashutosh|  27|         3| 92000|\n",
      "|Hrushikesh|  25|         2| 87000|\n",
      "|     Myles|NULL|      NULL| 75000|\n",
      "|      NULL|  34|         8|157000|\n",
      "|      NULL|  37|      NULL|  NULL|\n",
      "+----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fdc06e",
   "metadata": {},
   "source": [
    "## Drop Null Values\n",
    "### When we use na it will drop the rows that have a null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "007a1e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|      Name|Age|Experience|Salary|\n",
      "+----------+---+----------+------+\n",
      "|    Tushti| 25|         3| 90000|\n",
      "|      Neha| 29|         5|118000|\n",
      "|    Swathi| 24|         1| 85000|\n",
      "|     Rohan| 25|         2| 98000|\n",
      "|  Ashutosh| 27|         3| 92000|\n",
      "|Hrushikesh| 25|         2| 87000|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83197e4c",
   "metadata": {},
   "source": [
    "### if you want to drop rows that have all values as null, you use how='all', by default how='any'. Since we don't have any rows with all null values we won't see any change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d6701625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+\n",
      "|      Name| Age|Experience|Salary|\n",
      "+----------+----+----------+------+\n",
      "|    Tushti|  25|         3| 90000|\n",
      "|      Neha|  29|         5|118000|\n",
      "|    Swathi|  24|         1| 85000|\n",
      "|     Rohan|  25|         2| 98000|\n",
      "|  Ashutosh|  27|         3| 92000|\n",
      "|Hrushikesh|  25|         2| 87000|\n",
      "|     Myles|NULL|      NULL| 75000|\n",
      "|      NULL|  34|         8|157000|\n",
      "|      NULL|  37|      NULL|  NULL|\n",
      "+----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7310d0",
   "metadata": {},
   "source": [
    "## Threshold\n",
    "### Thresh = 2 means atleast there should be 2 non null values in a row otherwise drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c7ebf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+\n",
      "|      Name| Age|Experience|Salary|\n",
      "+----------+----+----------+------+\n",
      "|    Tushti|  25|         3| 90000|\n",
      "|      Neha|  29|         5|118000|\n",
      "|    Swathi|  24|         1| 85000|\n",
      "|     Rohan|  25|         2| 98000|\n",
      "|  Ashutosh|  27|         3| 92000|\n",
      "|Hrushikesh|  25|         2| 87000|\n",
      "|     Myles|NULL|      NULL| 75000|\n",
      "|      NULL|  34|         8|157000|\n",
      "+----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.na.drop(how='any',thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5abf6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|      Name|Age|Experience|Salary|\n",
      "+----------+---+----------+------+\n",
      "|    Tushti| 25|         3| 90000|\n",
      "|      Neha| 29|         5|118000|\n",
      "|    Swathi| 24|         1| 85000|\n",
      "|     Rohan| 25|         2| 98000|\n",
      "|  Ashutosh| 27|         3| 92000|\n",
      "|Hrushikesh| 25|         2| 87000|\n",
      "|      NULL| 34|         8|157000|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.na.drop(how='any',thresh=3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f077f",
   "metadata": {},
   "source": [
    "### The row 'Myles' was dropped because it has only 2 non null values but we want atleast 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d50ec",
   "metadata": {},
   "source": [
    "## Subset\n",
    "### Will drop a row with null values from a particular column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8708ed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|      Name|Age|Experience|Salary|\n",
      "+----------+---+----------+------+\n",
      "|    Tushti| 25|         3| 90000|\n",
      "|      Neha| 29|         5|118000|\n",
      "|    Swathi| 24|         1| 85000|\n",
      "|     Rohan| 25|         2| 98000|\n",
      "|  Ashutosh| 27|         3| 92000|\n",
      "|Hrushikesh| 25|         2| 87000|\n",
      "|      NULL| 34|         8|157000|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.na.drop(how='any',subset=['Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bec7e967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+\n",
      "|      Name| Age|Experience|Salary|\n",
      "+----------+----+----------+------+\n",
      "|    Tushti|  25|         3| 90000|\n",
      "|      Neha|  29|         5|118000|\n",
      "|    Swathi|  24|         1| 85000|\n",
      "|     Rohan|  25|         2| 98000|\n",
      "|  Ashutosh|  27|         3| 92000|\n",
      "|Hrushikesh|  25|         2| 87000|\n",
      "|     Myles|NULL|      NULL| 75000|\n",
      "+----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.na.drop(how='any',subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fea3692",
   "metadata": {},
   "source": [
    "## Fill missing values\n",
    "### Will only fill 'Missing Value' for those column with dtype as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04c3f14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+\n",
      "|         Name| Age|Experience|Salary|\n",
      "+-------------+----+----------+------+\n",
      "|       Tushti|  25|         3| 90000|\n",
      "|         Neha|  29|         5|118000|\n",
      "|       Swathi|  24|         1| 85000|\n",
      "|        Rohan|  25|         2| 98000|\n",
      "|     Ashutosh|  27|         3| 92000|\n",
      "|   Hrushikesh|  25|         2| 87000|\n",
      "|        Myles|NULL|      NULL| 75000|\n",
      "|Missing Value|  34|         8|157000|\n",
      "|Missing Value|  37|      NULL|  NULL|\n",
      "+-------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.na.fill('Missing Value').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "700c7bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+\n",
      "|      Name| Age|Experience|Salary|\n",
      "+----------+----+----------+------+\n",
      "|    Tushti|  25|         3| 90000|\n",
      "|      Neha|  29|         5|118000|\n",
      "|    Swathi|  24|         1| 85000|\n",
      "|     Rohan|  25|         2| 98000|\n",
      "|  Ashutosh|  27|         3| 92000|\n",
      "|Hrushikesh|  25|         2| 87000|\n",
      "|     Myles|NULL|         7| 75000|\n",
      "|      NULL|  34|         8|157000|\n",
      "|      NULL|  37|         7|  NULL|\n",
      "+----------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.na.fill(7,'Experience').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61830a9",
   "metadata": {},
   "source": [
    "### Replace null value using mean of a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0738e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c251a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols =['Age','Experience','Salary'],\n",
    "    outputCols = [\"{}_imputed\".format(c) for c in ['Age','Experience','Salary']]\n",
    "                ).setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "410a74b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+-----------+------------------+--------------+\n",
      "|      Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+----------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Tushti|  25|         3| 90000|         25|                 3|         90000|\n",
      "|      Neha|  29|         5|118000|         29|                 5|        118000|\n",
      "|    Swathi|  24|         1| 85000|         24|                 1|         85000|\n",
      "|     Rohan|  25|         2| 98000|         25|                 2|         98000|\n",
      "|  Ashutosh|  27|         3| 92000|         27|                 3|         92000|\n",
      "|Hrushikesh|  25|         2| 87000|         25|                 2|         87000|\n",
      "|     Myles|NULL|      NULL| 75000|         28|                 3|         75000|\n",
      "|      NULL|  34|         8|157000|         34|                 8|        157000|\n",
      "|      NULL|  37|      NULL|  NULL|         37|                 3|        100250|\n",
      "+----------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_spark).transform(df_spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08cbbb3",
   "metadata": {},
   "source": [
    "### Replace null values with Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8882cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols =['Age','Experience','Salary'],\n",
    "    outputCols = [\"{}_imputed\".format(c) for c in ['Age','Experience','Salary']]\n",
    "                ).setStrategy('median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "713e8dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+------+-----------+------------------+--------------+\n",
      "|      Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+----------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Tushti|  25|         3| 90000|         25|                 3|         90000|\n",
      "|      Neha|  29|         5|118000|         29|                 5|        118000|\n",
      "|    Swathi|  24|         1| 85000|         24|                 1|         85000|\n",
      "|     Rohan|  25|         2| 98000|         25|                 2|         98000|\n",
      "|  Ashutosh|  27|         3| 92000|         27|                 3|         92000|\n",
      "|Hrushikesh|  25|         2| 87000|         25|                 2|         87000|\n",
      "|     Myles|NULL|      NULL| 75000|         25|                 3|         75000|\n",
      "|      NULL|  34|         8|157000|         34|                 8|        157000|\n",
      "|      NULL|  37|      NULL|  NULL|         37|                 3|         90000|\n",
      "+----------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_spark).transform(df_spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef1c75",
   "metadata": {},
   "source": [
    "## Filter Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3d3d630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sparkpy = spark.read.csv('test3.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "72502c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|      Name|Age|Experience|Salary|\n",
      "+----------+---+----------+------+\n",
      "|    Tushti| 25|         3| 90000|\n",
      "|      Neha| 29|         5|118000|\n",
      "|    Swathi| 24|         1| 85000|\n",
      "|     Rohan| 25|         2| 98000|\n",
      "|  Ashutosh| 27|         3| 92000|\n",
      "|Hrushikesh| 25|         2| 87000|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sparkpy.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d3a9f",
   "metadata": {},
   "source": [
    "### Salary of People less than or equal to 95000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "61870b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|      Name|Age|Experience|Salary|\n",
      "+----------+---+----------+------+\n",
      "|    Tushti| 25|         3| 90000|\n",
      "|    Swathi| 24|         1| 85000|\n",
      "|  Ashutosh| 27|         3| 92000|\n",
      "|Hrushikesh| 25|         2| 87000|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sparkpy.filter(\"Salary<=95000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "17cd8e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|      Name|Age|\n",
      "+----------+---+\n",
      "|    Tushti| 25|\n",
      "|    Swathi| 24|\n",
      "|  Ashutosh| 27|\n",
      "|Hrushikesh| 25|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sparkpy.filter(\"Salary<=95000\").select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77174f7",
   "metadata": {},
   "source": [
    "### 2 or more conditions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "393fe7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+------+\n",
      "|      Name|Age|Experience|Salary|\n",
      "+----------+---+----------+------+\n",
      "|    Tushti| 25|         3| 90000|\n",
      "|  Ashutosh| 27|         3| 92000|\n",
      "|Hrushikesh| 25|         2| 87000|\n",
      "+----------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sparkpy.filter((df_sparkpy['Salary']<=95000) & (df_sparkpy['Salary']>85000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2b93360e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|      Name|Age|\n",
      "+----------+---+\n",
      "|    Tushti| 25|\n",
      "|  Ashutosh| 27|\n",
      "|Hrushikesh| 25|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sparkpy.filter((df_sparkpy['Salary']<=95000) & (df_sparkpy['Salary']>85000)).select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c664e08e",
   "metadata": {},
   "source": [
    "### Not operation (~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f4a50926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|Age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "| Neha| 29|         5|118000|\n",
      "|Rohan| 25|         2| 98000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sparkpy.filter(~(df_sparkpy['Salary']<=95000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4135b42",
   "metadata": {},
   "source": [
    "## Group By and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "385727ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.csv('test4.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4eefc9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------+\n",
      "|Name|  Department|Salary|\n",
      "+----+------------+------+\n",
      "|Jane|Software Eng| 87000|\n",
      "|Jane|Data Science| 92000|\n",
      "|Liza|Software Eng|110000|\n",
      "| Ram|Software Eng| 98000|\n",
      "|Jane|    Big Data| 75000|\n",
      "|Abey|Data Science| 93000|\n",
      "|Liza|    Big Data| 83000|\n",
      "| Ram|    Big Data| 77000|\n",
      "|Nick|Software Eng|125000|\n",
      "|Nick|Data Science| 90000|\n",
      "+----+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b82bd2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a80dea",
   "metadata": {},
   "source": [
    "### Find max Salary for all with respect to the Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dad5ff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|Name|sum(Salary)|\n",
      "+----+-----------+\n",
      "|Abey|      93000|\n",
      "|Liza|     193000|\n",
      "|Nick|     215000|\n",
      "| Ram|     175000|\n",
      "|Jane|     254000|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b4174",
   "metadata": {},
   "source": [
    "### Max Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1c9f4ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|Name|max(Salary)|\n",
      "+----+-----------+\n",
      "|Abey|      93000|\n",
      "|Liza|     110000|\n",
      "|Nick|     125000|\n",
      "| Ram|      98000|\n",
      "|Jane|      92000|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.groupBy('Name').max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e4ff5",
   "metadata": {},
   "source": [
    "### Min Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "13c9df88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|Name|min(Salary)|\n",
      "+----+-----------+\n",
      "|Abey|      93000|\n",
      "|Liza|      83000|\n",
      "|Nick|      90000|\n",
      "| Ram|      77000|\n",
      "|Jane|      75000|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.groupBy('Name').min().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae474f4c",
   "metadata": {},
   "source": [
    "### Group by Salary on which department pays highest salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0e6c67e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  Department|sum(Salary)|\n",
      "+------------+-----------+\n",
      "|Software Eng|     420000|\n",
      "|    Big Data|     235000|\n",
      "|Data Science|     275000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.groupBy('Department').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57be0b0",
   "metadata": {},
   "source": [
    "### Find the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "762bc122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|  Department|      avg(Salary)|\n",
      "+------------+-----------------+\n",
      "|Software Eng|         105000.0|\n",
      "|    Big Data|78333.33333333333|\n",
      "|Data Science|91666.66666666667|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.groupBy('Department').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde5623",
   "metadata": {},
   "source": [
    "### Find the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a0d4abc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|  Department|count|\n",
      "+------------+-----+\n",
      "|Software Eng|    4|\n",
      "|    Big Data|    3|\n",
      "|Data Science|    3|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.groupBy('Department').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf484c4",
   "metadata": {},
   "source": [
    "### Total Salary using Aggregrate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e197ec3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|     930000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42391fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
